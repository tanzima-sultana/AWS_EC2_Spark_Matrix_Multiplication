###############################################
#                CONFIG TEMPLATE
#  Copy this file and rename as config.env
#      cp config.env.template config.env
###############################################

############# PYTHON SETTINGS #################

# Local Python version
PYTHON="python3"

# Local Python virtual environment (on your laptop)
PY_ENV_LOCAL="/path/to/local/python-env"

# Python path on EC2 (inside the cluster)
PYTHON_PATH="/home/ubuntu/pyspark-env/bin/python"


############# SPARK SETTINGS #################

# Extra Spark packages (e.g., S3 support)
PACKAGES="org.apache.spark:spark-hadoop-cloud_2.13:4.0.1"

# Path to Spark job on EC2
SPARK_FILE="/home/ubuntu/PROJECT_NAME/matrix_mul.py"


############# PROJECT PATHS ###################

# Local project folder (on your laptop)
PROJECT_PATH_LOCAL="/path/to/local/folder"

# Remote folder on EC2 where project is copied
PROJECT_PATH_CLOUD="/home/ubuntu"

# Folder name of project (repo root)
SPARK_MATRIX_MUL_PROJECT="PROJECT_NAME"


############# AWS SSH KEY #####################

# Path to SSH key on your laptop
KEY_PATH_LOCAL="/path/to/local/key/folder"

# Path to SSH key on EC2 (usually /home/ubuntu)
KEY_PATH_CLOUD="/home/ubuntu"

# Name of the .pem key used for SSH
KEY="your-key.pem"


############# S3 BUCKET #######################

# S3 bucket used for storing matrix files
BUCKET="your-s3-bucket-name"


############# EC2 IP ADDRESSES ################

# Master node public IP (for browser access + SSH)
MASTER_PUBLIC_IP="X.X.X.X"

# Master node private IP (for Spark internal comms)
MASTER_PRIVATE_IP="172.xx.xx.xx"

# Worker node private IPs (for Spark workers)
WORKER_1_PRIVATE_IP="172.xx.xx.xx"
WORKER_2_PRIVATE_IP="172.xx.xx.xx"

# Full Spark master URL (private IP is required)
MASTER="spark://172.xx.xx.xx:7077"


###############################################
# Save this file as config.env and fill in
# all fields before running the scripts.
###############################################

